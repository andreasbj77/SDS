---
title: "Predicting diseases in cattle herds"
author: "Andreas Joergensen, Cathrine Olsen, Louise Christoffersen & Mette Moeller"
date: "8-1-2021"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    theme: flatly
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	fig.align = "center")
  rm(list=ls())
  Sys.setenv(LANG = "en")
  options(scipen = 5)
```


```{r}
library(readr)
library(tidyverse)
library(magrittr)
library(GGally)
library(tidymodels)
library(mlbench)
library(vip)
library(ranger)
library(ggpubr)
library(themis)
library(naniar)
library(ggplot2)
library(ggpubr) #For mixing multiple plots together
```

Loading the data. 
```{r}
data <- read_csv("https://raw.githubusercontent.com/andreasbj77/SDS/main/Project/Cows.csv")
```

```{r}
data %>% glimpse()
```

```{r}
data %>%
  count(DiseaseCategory)
```


After loading the data set some variables are recoded as factors. 
```{r}
data %<>%
  mutate(HerdNumber = HerdNumber %>% as.factor()) %>%
  mutate(Race = Race %>% as.factor()) %>%
  mutate(Insemination_month = Insemination_month %>% as.factor()) %>%
  mutate(Calving_month = Calving_month %>% as.factor())
```


# EDA
Looking at the distribution of the newly created variable, *DiseaseCategory*.

```{r}
data %>%
  ggplot(aes(x = DiseaseCategory)) +
  geom_bar(fill ="tomato", alpha = 0.8) +
  geom_text(aes(label=..count..), stat = "count", position=position_stack(0.5)) +
  coord_flip() 
```

## Herd and race

Looking at the distribution of the *HerdNumber* variable.
```{r}
 data %>%
  count(HerdNumber) %>%
  mutate(pct = (n/sum(n) * 100)%>% round(3)) %>%
  mutate(HerdNumber = HerdNumber %>% as.factor()) 
```


```{r}
data %>%
  count(DiseaseCategory, HerdNumber) %>%
  group_by(HerdNumber) %>%
  mutate(pct = (n/sum(n))%>% round(3)) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  mutate(ypos = cumsum(pct)- 0.5*pct ) %>%
  ungroup() %>%
  mutate(pct = pct *100) %>%
  ggplot(aes(x = DiseaseCategory, y = pct, fill = HerdNumber)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Percentage")  + facet_wrap(~HerdNumber, ncol = 2) +
  coord_flip()
```

```{r}
data%>%
  count(Race) %>%
  mutate(pct = (n/sum(n)*100) %>%round(3))
```


```{r}
data %>%
  count(DiseaseCategory, Race) %>%
  group_by(Race) %>%
  mutate(pct = (n/sum(n))%>% round(3)) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  mutate(ypos = cumsum(pct)- 0.5*pct ) %>%
  ungroup() %>%
  mutate(pct = pct *100) %>%
  ggplot(aes(x = DiseaseCategory, y = pct, fill = Race)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Percentage")  + facet_wrap(~Race, ncol = 2) +
  coord_flip()
```


```{r}
data %>%
  count(HerdNumber, Race) %>%
  group_by(HerdNumber) %>%
  mutate(pct = (n / sum(n, na.rm = TRUE)*100) %>% round(3) ) %>%
  select(-n) %>%
  pivot_wider(names_from = Race, values_from = pct)
```

## Insemination

Looking at the distribution of the *Insemination_month* variable.
```{r}
 data %>%
  count(Insemination_month) %>%
  mutate(pct = (n/sum(n))*100 %>% round(3)) %>%
  mutate(Insemination_month = Insemination_month %>% as.factor())
```

```{r}
data %>%
  count(InseminationNumber)
```
Plotting diseases against insemination number in percentage of the whole population

```{r fig.height=6, fig.width=10}
data %>%
  group_by(InseminationNumber) %>%
  count(DiseaseCategory, InseminationNumber) %>%
  mutate(pct = (n/sum(n))) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  ungroup() %>%
  mutate(pct = pct *100) %>%
    ggplot(aes(x = DiseaseCategory, y = pct, fill = InseminationNumber)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "Percentage")  + facet_wrap(~InseminationNumber, ncol = 3) +
    coord_flip()
```


```{r}
data %>%
  group_by(InseminationNumber) %>%
  count(DiseaseCategory, InseminationNumber) %>%
  mutate(pct = (n/sum(n)*100) %>% round(3)) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  ungroup() %>%
  arrange(DiseaseCategory)
```


## Lactation and calving

Looking at the distribution of the *Calving_month* variable.
```{r}
data %>%
  count(Calving_month) %>%
  mutate(pct = (n/sum(n))*100 %>% round(3)) %>%
  mutate(Calving_month = Calving_month %>% as.factor())
```


```{r}
data %>%
  count(LactationNumber)
```

Plotting diseases against lactation number in percentage of the whole population

```{r fig.height=6, fig.width=10}
data %>%
  group_by(LactationNumber) %>%
  count(DiseaseCategory, LactationNumber) %>%
  mutate(pct = (n/sum(n))) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  ungroup() %>%
  mutate(pct = pct *100) %>%
  ggplot(aes(x = DiseaseCategory, y = pct, fill = LactationNumber)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Percentage")  + facet_wrap(~LactationNumber, ncol = 3) +
  coord_flip()
```


```{r}
data %>%
  group_by(LactationNumber) %>%
  count(DiseaseCategory, LactationNumber) %>%
  mutate(pct = (n/sum(n)*100) %>% round(3)) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor()) %>%
  ungroup() %>%

  arrange(DiseaseCategory)
```
 
## Milk yield

```{r}
data %>%
  mutate(MilkYield = MilkYield %>% round(0)) %>%
  count(MilkYield) %>%
  mutate(pct = (n/sum(n)) %>% round(4)) %>%
    ungroup() %>%
 mutate(pct = pct *100) %>%
  filter(MilkYield == 0)
```


```{r}
data %>%
 filter(MilkYield != 0) %>%
  mutate(MilkYield = MilkYield %>% round(0)) %>%
  count(MilkYield) %>%
  mutate(pct = (n/sum(n)) %>% round(4)) %>%
    ungroup() %>%
 mutate(pct = pct *100) %>%
 ggplot(aes(x = MilkYield, y = pct,)) +
 geom_col(show.legend = FALSE, fill = "#0072B2") +
 labs(x = "Litres of milk", y = "Percentage")
```


```{r}
data %>%
  filter(Change_milkyield !=0) %>%
  mutate(Change_milkyield = Change_milkyield %>% round(0)) %>%
  count(Change_milkyield) %>%
  mutate(pct = (n/sum(n)) %>% round(4)) %>%
    ungroup() %>%
 mutate(pct = pct *100) %>%
 ggplot(aes(x = Change_milkyield, y = pct,)) +
 geom_col(show.legend = FALSE, fill = "#0072B2") +
 labs(x = "Change in milk yield", y = "Percentage")
```






```{r}
plot <- data %>%
  filter(!(MilkYield == "NA")) %>%
  mutate(MilkYield = MilkYield %>% as.numeric()) %>%
  mutate(MilkYield1 = ifelse(MilkYield > 60 , "]60;∞[",
                            ifelse(MilkYield<= 60 & MilkYield > 50, "]50;60]",
                            ifelse(MilkYield <= 50 & MilkYield > 40, "]40;50]", 
                            ifelse(MilkYield <= 40 & MilkYield > 30, "]30;40]",
                            ifelse(MilkYield <= 30 & MilkYield > 20, "]20;30]", 
                            ifelse(MilkYield <= 20 & MilkYield > 10, "]10;20]",  
                            ifelse(MilkYield <= 10 & MilkYield >= 0, "[0;10]", 0)))))))) %>%
            mutate(MilkYield1 = MilkYield1 %>% as.factor)  %>%
     group_by(DiseaseCategory) %>%
    add_count(MilkYield1) %>%
  distinct(MilkYield1, n, .keep_all = TRUE)

plot %>%
 ggplot(aes(x = MilkYield1, y = n, fill = DiseaseCategory)) +
geom_col(show.legend = TRUE, position = position_dodge()) +
  theme(legend.position = c(.85,.78))  +
 labs(x = "Litres of milk", y = "Number") 
```


```{r}
plot1 <- data %>%
  mutate(Change_milkyield = Change_milkyield %>% as.numeric()) %>%
  mutate(Change_milkyield1 = ifelse(Change_milkyield > 40 , 6,
                            ifelse(Change_milkyield <= 40 & Change_milkyield > 20, 5, 
                            ifelse(Change_milkyield <= 20 & Change_milkyield > 0, 4,
                            ifelse(Change_milkyield <= 0 & Change_milkyield > -20, 3, 
                            ifelse(Change_milkyield <= -20 & Change_milkyield > -40, 2, 
                            ifelse(Change_milkyield <= -40 , 1, 0))))))) %>%
  group_by(DiseaseCategory) %>%
  add_count(Change_milkyield1) %>%
  distinct(Change_milkyield1, n, .keep_all = TRUE)

 
plot1$Change_milkyield1 <- factor(as.numeric(plot1$Change_milkyield1), #Next the factor command assigns names to the intervals of grades
          levels = c(1, 2, 3,  4, 5, 6), 
          labels = c("]-∞;-40]",  "]-40;-20]", "]-20;0]", "]0;20]", "]20;40]","]40;∞["))

 


plot1 %>%
 ggplot(aes(x = Change_milkyield1, y = n, fill = DiseaseCategory)) +
 geom_col(show.legend = TRUE, position = position_dodge()) +
   theme(legend.position = c(.85,.78))  +
 labs(x = "Change in milk yield", y = "Number") 
```


Removing unneeded data.
```{r}
rm(calvingpie, calvingpie_plot, herdpie, insimpie, insimpie_plot, plot, plot1)
```





## NAs

For all disease types.
```{r}
missing.values <- data %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)


levels <-
    (missing.values  %>% filter(isna == T) %>% arrange(desc(pct)))$key

percentage.plot <- missing.values %>%
      ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), 
                 stat = 'identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
      coord_flip() +
      labs(title = "Percentage of missing values", x =
             'Variable', y = "% of missing values")

percentage.plot
```

```{r}
rm(missing.values, percentage.plot, levels)
```


### NA's for DiseaseCategory


Creating a temporary data set containing clinical parameters and DiseaseCategory.
```{r}
clin <- data[,4:17]
clin %<>% add_column(data[,24])
```



```{r fig.height=6, fig.width=10}
NA_clin <- gg_miss_var(clin, facet = DiseaseCategory, show_pct = TRUE)
NA_clin
```

```{r}
data %>%
  group_by(DiseaseCategory) %>%
  add_count(DiseaseCategory) %>%
  mutate(Na_keton = ((sum(is.na(Ketonstof_urin))/n)*100) %>% round(1)) %>%
  mutate(Na_skede = ((sum(is.na(Skede))/n)*100) %>% round(1)) %>%
  mutate(Na_goedning = ((sum(is.na(Goedning))/n)*100) %>% round(1)) %>%
  mutate(Na_boer = ((sum(is.na(Boer))/n)*100) %>% round(1)) %>%
  mutate(Na_huld = ((sum(is.na(Huld))/n)*100) %>% round(1)) %>%
  distinct(DiseaseCategory, n, .keep_all = TRUE) %>%
  mutate(Na_keton = paste(Na_keton, "%")) %>%
  mutate(Na_skede = paste(Na_skede, "%")) %>%
  mutate(Na_goedning = paste(Na_goedning, "%")) %>%
  mutate(Na_boer = paste(Na_boer, "%"))%>%
  mutate(Na_huld = paste(Na_huld, "%"))%>%
  arrange(desc(n)) %>%
  select(DiseaseCategory, Na_keton, Na_skede, Na_goedning, Na_boer, Na_huld)
```


```{r}
rm(clin, NA_clin, NA_plot)
```


# Supervised Machine Learning

After loading the data set some variables are recoded as factors. 

```{r}
data %<>%
  select(-c(CMT_min_1_kirtel, Rejseadfaerd, Halthed, Hygiejne_yver, Hygiejne_haser, Hud_Haarlag, Hasetrykning, Andre_trykninger, Flugtafstand))
```


```{r}
data %<>%
  mutate(Huld = Huld %>% as.factor()) %>%
  mutate(Boer = Boer %>% as.factor()) %>%
  mutate(Skede = Skede %>% as.factor()) %>%
  mutate(Goedning = Goedning %>% as.factor()) %>%
  mutate(Ketonstof_urin = Ketonstof_urin %>% as.factor()) %>%
  mutate(DiseaseCategory = DiseaseCategory %>% as.factor())
```

```{r}
data %>% glimpse()
```


## Data sets for modeling

Three different ways of categorizing is used and training and test splits are created for each data set.

**5 categories**
```{r}
data_five <- data %>%
  select(-Disease) %>%
  rename(y = DiseaseCategory)
```

```{r}
data_split_five <- data_five %>% initial_split(prop = 0.8, strata = y)

data_train_five <- data_split_five %>% training()
data_test_five <- data_split_five %>% testing()
```

**3 categories**
```{r}
data_three <- data %>%
  select(-Disease) %>%
  mutate(DiseaseCategory = ifelse(DiseaseCategory == "Healthy", "Healthy",
                  ifelse(DiseaseCategory == "Udder disorders", "Udder disorders", "Other")) %>% as.factor()) %>%
  rename(y = DiseaseCategory) 
```

```{r}
data_split_three <- data_three %>% initial_split(prop = 0.8, strata = y)

data_train_three <- data_split_three %>% training()
data_test_three <- data_split_three %>% testing()
```

**2 categories**
```{r}
data_bi <- data %>%
  select(-Disease) %>%
  mutate(DiseaseCategory = ifelse(DiseaseCategory == "Healthy", "Healthy", "Sick") %>% as.factor()) %>%
  rename(y = DiseaseCategory) 
```

```{r}
data_split_bi <- data_bi %>% initial_split(prop = 0.8, strata = y)

data_train_bi <- data_split_bi %>% training()
data_test_bi <- data_split_bi %>% testing()
```

```{r}
glimpse(data_five)
```


## Recipies

A recipe is created for each data set where NAs are imputed using k nearest neighbor imputation, numeric variables are centered and scaled, and the data is upsampled to overcome the uneven classes.
```{r}
set.seed(1337)
# 5 categories
data_recipe_five <- data_train_five %>%
  recipe(y ~.) %>%
  step_upsample(y) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 

# 3 categories
data_recipe_three <- data_train_three %>%
  recipe(y ~.) %>%
  step_upsample(y) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 

# 2 categories
data_recipe_bi <- data_train_bi %>%
  recipe(y ~.) %>%
  step_upsample(y) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

## Model creation

Two models are used for the Supervised Machine Learning part - a decision tree and a random forest model. The models are defined below.
```{r}
model_dt <- decision_tree(mode = 'classification',
                          cost_complexity = tune(),
                          tree_depth = tune(), 
                          min_n = tune()
                          ) %>%
  set_engine('rpart') 

model_rf <- rand_forest(mode = 'classification',
                        trees = 25, #number of trees choosen for the ensemble
                        mtry = tune(),
                        min_n = tune()
                        ) %>%
  set_engine('ranger', importance = 'impurity')
```

After defining the models the recipes from earlier are combined with the different models to create a workflow for each separate model.
```{r} 
# 5 categories
workflow_general_five <- workflow() %>% #Adding recipe to the general workflow
  add_recipe(data_recipe_five) 

workflow_rf_five <- workflow_general_five %>% #Adding the models to the workflow
  add_model(model_rf)

workflow_dt_five <- workflow_general_five %>%
  add_model(model_dt)

# 3 categories
workflow_general_three <- workflow() %>% 
  add_recipe(data_recipe_three) 

workflow_rf_three <- workflow_general_three %>% 
  add_model(model_rf)

workflow_dt_three <- workflow_general_three %>%
  add_model(model_dt)

# 2 categories
workflow_general_bi <- workflow() %>% 
  add_recipe(data_recipe_bi) 

workflow_rf_bi <- workflow_general_bi %>% 
  add_model(model_rf)

workflow_dt_bi <- workflow_general_bi %>%
  add_model(model_dt)
```

The training data is then separated into 3 folds and training is repeated 3 times for all three categorizations. 
```{r}
set.seed(1337)
# 5 categories
data_resample_five <- data_train_five %>% 
  vfold_cv(strata = y,
           v = 3,
           repeats = 3)

# 3 categories
data_resample_three <- data_train_three %>% 
  vfold_cv(strata = y,
           v = 3, 
           repeats = 3)

# 2 categories
data_resample_bi <- data_train_bi %>% 
  vfold_cv(strata = y,
           v = 3,
           repeats = 3)
```


## Hyperparameter tuning

Since the models weren't designed with specific parameter values, the parameters are tuned with the `tune_grid()`  command for the three categorizations.
```{r}
set.seed(1337)
# 5 categories
tune_dt_five <-
  tune_grid(
    workflow_dt_five,
    resamples = data_resample_five,
    grid = 10
  )
```

```{r}
set.seed(1337)
tune_rf_five <-
  tune_grid(
    workflow_rf_five,
    resamples = data_resample_five,
    grid = 10
  )
```

```{r}
set.seed(1337)
# 3 categories
tune_dt_three <-
  tune_grid(
    workflow_dt_three,
    resamples = data_resample_three,
    grid = 10
  )
```

```{r}
set.seed(1337)
tune_rf_three <-
  tune_grid(
    workflow_rf_three,
    resamples = data_resample_three,
    grid = 10
  )
```

```{r}
set.seed(1337)
# 2 categories
tune_dt_bi <-
  tune_grid(
    workflow_dt_bi,
    resamples = data_resample_bi,
    grid = 10
  )
```

```{r}
set.seed(1337)
tune_rf_bi <-
  tune_grid(
    workflow_rf_bi,
    resamples = data_resample_five,
    grid = 10
  )
```


Since this is a classification problem the best model is chosen based on the 'roc_auc' metric for all the models.
```{r}
# 5 categories
best_param_dt_five <- tune_dt_five %>% select_best(metric = 'roc_auc')
best_param_rf_five <- tune_rf_five %>% select_best(metric = "roc_auc")

# 3 categories
best_param_dt_three <- tune_dt_three %>% select_best(metric = 'roc_auc')
best_param_rf_three <- tune_rf_three %>% select_best(metric = "roc_auc")

# 2 categories
best_param_dt_bi <- tune_dt_bi %>% select_best(metric = 'roc_auc')
best_param_rf_bi <- tune_rf_bi %>% select_best(metric = "roc_auc")
```



## Choosing the best model

After tuning the hyperparameters, the final workflows are created.
```{r}
# 5 categories
workflow_final_rf_five <- workflow_rf_five %>%
  finalize_workflow(parameters = best_param_rf_five) #adding the tuning to the existing workflow

workflow_final_dt_five <- workflow_dt_five %>%
  finalize_workflow(parameters = best_param_dt_five)

# 3 categories
workflow_final_rf_three <- workflow_rf_three %>%
  finalize_workflow(parameters = best_param_rf_three)

workflow_final_dt_three <- workflow_dt_three %>%
  finalize_workflow(parameters = best_param_dt_three)

# 2 categories
workflow_final_rf_bi <- workflow_rf_bi %>%
  finalize_workflow(parameters = best_param_rf_five) 

workflow_final_dt_bi <- workflow_dt_bi %>%
  finalize_workflow(parameters = best_param_dt_bi)
```

The final models are fitted to the training data.
```{r}
set.seed(1337)
# 5 categories
fit_rf_five <- workflow_final_rf_five %>%
  fit(data_train_five)

fit_dt_five <- workflow_final_dt_five %>%
  fit(data_train_five)

# 3 categories
fit_rf_three <- workflow_final_rf_three %>%
  fit(data_train_three)

fit_dt_three <- workflow_final_dt_three %>%
  fit(data_train_three)

# 2 categories
fit_rf_bi <- workflow_final_rf_bi %>%
  fit(data_train_bi)

fit_dt_bi <- workflow_final_dt_bi %>%
  fit(data_train_bi)
```

### Comparing the models

Tables are created by comparing the predicted values to the true values to evaluate the models performance.
```{r}
set.seed(1337)
# 5 categories
pred_collected_five <- tibble(
  truth = data_train_five %>% pull(y) %>% as.factor(), #Takes the true values from the training data
  rf = fit_rf_five %>% predict(new_data = data_train_five) %>% pull(.pred_class), #predicting y based on the training data
  dt = fit_dt_five %>% predict(new_data = data_train_five) %>% pull(.pred_class),) %>% 
  pivot_longer(cols = -truth, names_to = 'model', values_to = '.pred')

# 3 categories
pred_collected_three <- tibble(
  truth = data_train_three %>% pull(y) %>% as.factor(),
  rf = fit_rf_three %>% predict(new_data = data_train_three) %>% pull(.pred_class),
  dt = fit_dt_three %>% predict(new_data = data_train_three) %>% pull(.pred_class),) %>% 
  pivot_longer(cols = -truth, names_to = 'model', values_to = '.pred')


# 2 categories
pred_collected_bi <- tibble(
  truth = data_train_bi %>% pull(y) %>% as.factor(), 
  rf = fit_rf_bi %>% predict(new_data = data_train_bi) %>% pull(.pred_class), 
  dt = fit_dt_bi %>% predict(new_data = data_train_bi) %>% pull(.pred_class),) %>% 
  pivot_longer(cols = -truth, names_to = 'model', values_to = '.pred')
```

The accuracy of the models is calculated.
```{r}
# 5 categories
pred5 <- pred_collected_five %>%
  group_by(model) %>%
  accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  rename("5 categories" = .estimate)

# 3 categories
pred3 <- pred_collected_three %>%
  group_by(model) %>%
  accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  rename("3 categories" = .estimate) %>%
  select("3 categories")

# 5 categories
pred2 <- pred_collected_bi %>%
  group_by(model) %>%
  accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  rename("2 categories" = .estimate) %>%
  select("2 categories")

cbind(pred5, pred3, pred2)
```

Confusion matrices for all of the models are created.
```{r fig.height=6, fig.width=12}
# 5 categories
cm_rf_five <- pred_collected_five %>% conf_mat(truth, .pred)
heatmap5 <- cm_rf_five %>% autoplot(type = "heatmap")

# 3 categories
cm_rf_three <- pred_collected_three %>% conf_mat(truth, .pred)
heatmap3 <- cm_rf_three %>% autoplot(type = "heatmap")

# 2 categories
cm_rf_bi <- pred_collected_bi %>% conf_mat(truth, .pred)
heatmap2 <- cm_rf_bi %>% autoplot(type = "heatmap")

heatmap5
ggarrange(heatmap3, heatmap2)
```


## Fitting the last models

The models are evaluated on the test set.
```{r}
set.seed(1337)
# 5 categories
fit_last_rf_five <- workflow_final_rf_five %>% last_fit(split = data_split_five)

# 3 categories
fit_last_rf_three <- workflow_final_rf_three %>% last_fit(split = data_split_three)

# 5 categories
fit_last_rf_bi <- workflow_final_rf_bi %>% last_fit(split = data_split_bi)
```

Plots showing variables of importance are presented for all of the models.
```{r fig.height=6, fig.width=10}
# 5 categories
vip5 <- fit_last_rf_five %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)

# 3 categories
vip3 <- fit_last_rf_three %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)

# 2 categories
vip2 <- fit_last_rf_bi %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)

ggarrange(vip5, vip3, vip2)
```

## Evaluation

The accuracy of the models performance on the test set is calculated.
```{r}
test_metrics5 <- fit_last_rf_five %>% collect_metrics() %>%
  rename("5 categories" = .estimate) %>%
  select(-.estimator)

test_metrics3 <- fit_last_rf_three %>% collect_metrics() %>%
    rename("3 categories" = .estimate) %>%
  select("3 categories")

test_metrics2 <- fit_last_rf_bi %>% collect_metrics() %>%
    rename("2 categories" = .estimate) %>%
  select("2 categories")
    
cbind(test_metrics5, test_metrics3, test_metrics2)
```

Based on the predictions confusion matrices are presented.
```{r fig.height=6, fig.width=12}
# 5 categories
test_pred5 <- fit_last_rf_five %>% collect_predictions() 

mat5 <- test_pred5 %>% conf_mat(y, .pred_class) 
heatplot5 <- mat5 %>% autoplot(type = "heatmap")

# 3 categories
test_pred3 <- fit_last_rf_three %>% collect_predictions() 

mat3 <- test_pred3 %>% conf_mat(y, .pred_class) 
heatplot3 <- mat3 %>% autoplot(type = "heatmap")

# 2 categories
test_pred2 <- fit_last_rf_bi %>% collect_predictions() 

mat2 <- test_pred2 %>% conf_mat(y, .pred_class) 
heatplot2 <- mat2 %>% autoplot(type = "heatmap")

ggarrange(heatplot2, heatplot3)
heatplot5
```
Since the overall accuracy can be misleading in cases with unbalanced classes the precision for each class in each of the categorizations is calculated.

```{r}
cm5 <- as.matrix(mat5$table)
cm3 <- as.matrix(mat3$table)
cm2 <- as.matrix(mat2$table)
```

```{r}
rowsums5 = apply(cm5, 1, sum) #Number of predictions per class
colsums5 = apply(cm5, 2, sum) #Number of predictions per class
diag5 = diag(cm5) # Number of correctly classified instances per class

Precision5 = diag5/rowsums5 %>% as.table()
Sensitivity5 = diag5/colsums5 %>% as.table()

cbind(Precision5, Sensitivity5)
```

```{r}
rowsums3 = apply(cm3, 1, sum)
colsums3 = apply(cm3, 2, sum) #Number of predictions per class
diag3 = diag(cm3) 

Precision3 = diag3/rowsums3 %>% as.table()
Sensitivity3 = diag3/colsums3 %>% as.table()

cbind(Precision3, Sensitivity3)
```



```{r}
rowsums2 = apply(cm2, 1, sum)
colsums2 = apply(cm2, 2, sum) #Number of predictions per class
diag2 = diag(cm2)

Precision2 = diag2/rowsums2 %>% as.table()
Sensitivity2 = diag2/colsums2 %>% as.table()

cbind(Precision2, Sensitivity2)
```


# Neural Networks

## Data sets for modeling

Three different ways of categorizing is used and training and test splits are created for each data set.

**5 categories**
```{r}
data_five <- data %>%
  select(-Disease) %>%
  rename(y = DiseaseCategory)
```

```{r}
data_split_five <- data_five %>% initial_split(prop = 0.8, strata = y)

data_train_five <- data_split_five %>% training()
data_test_five <- data_split_five %>% testing()
```

**3 categories**
```{r}
data_three <- data %>%
  select(-Disease) %>%
  mutate(DiseaseCategory = ifelse(DiseaseCategory == "Healthy", "Healthy",
                  ifelse(DiseaseCategory == "Udder disorders", "Udder disorders", "Other")) %>% as.factor()) %>%
  rename(y = DiseaseCategory) 
```

```{r}
data_split_three <- data_three %>% initial_split(prop = 0.8, strata = y)

data_train_three <- data_split_three %>% training()
data_test_three <- data_split_three %>% testing()
```

**2 categories**
```{r}
data_bi <- data %>%
  select(-Disease) %>%
  mutate(DiseaseCategory = ifelse(DiseaseCategory == "Healthy", "Healthy", "Sick") %>% as.factor()) %>%
  rename(y = DiseaseCategory) 
```

```{r}
data_split_bi <- data_bi %>% initial_split(prop = 0.8, strata = y)

data_train_bi <- data_split_bi %>% training()
data_test_bi <- data_split_bi %>% testing()
```

```{r}
glimpse(data_five)
```


```{r}
library(keras)
```

## Recipes

```{r}
# 5 categories
data_recipe_DL_five <- data_train_five %>%
  recipe(y ~.) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  themis::step_upsample(y) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()

# 3 categories
data_recipe_DL_three <- data_train_three %>%
  recipe(y ~.) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  themis::step_upsample(y) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()

# 2 categories
data_recipe_DL_bi <- data_train_bi %>%
  recipe(y ~.) %>%
  step_knnimpute(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  themis::step_upsample(y) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()
```


## Defining X and Y variables

*x_train* and *x_test* are defined as containing only the explanatory stocks without the date variable. Simultaniously *y_train* and *y_test* are created containing the response variable - the lead of Apple as in the supervised learning part. 
```{r}
# 5 categories - train data
x_train_five <- juice(data_recipe_DL_five) %>% select(-starts_with('y')) %>% as.matrix()
y_train_five <- juice(data_recipe_DL_five) %>% select(starts_with('y')) %>% as.matrix()

# 5 categories - test data
x_test_five <- bake(data_recipe_DL_five, new_data = data_test_five) %>% select(-starts_with('y')) %>% as.matrix()
y_test_five <- bake(data_recipe_DL_five, new_data = data_test_five) %>% select(starts_with('y')) %>% as.matrix()

# 3 categories - train data
x_train_three <- juice(data_recipe_DL_three) %>% select(-starts_with('y')) %>% as.matrix()
y_train_three <- juice(data_recipe_DL_three) %>% select(starts_with('y')) %>% as.matrix()

# 3 categories - test data
x_test_three <- bake(data_recipe_DL_three, new_data = data_test_three) %>% select(-starts_with('y')) %>% as.matrix()
y_test_three <- bake(data_recipe_DL_three, new_data = data_test_three) %>% select(starts_with('y')) %>% as.matrix()

# 2 categories - train data
x_train_bi <- juice(data_recipe_DL_bi) %>% select(-starts_with('y')) %>% as.matrix()
y_train_bi <- juice(data_recipe_DL_bi) %>% select(starts_with('y')) %>% as.matrix()

# 2 categories - test data
x_test_bi <- bake(data_recipe_DL_bi, new_data = data_test_bi) %>% select(-starts_with('y')) %>% as.matrix()
y_test_bi <- bake(data_recipe_DL_bi, new_data = data_test_bi) %>% select(starts_with('y')) %>% as.matrix()
```

## Models

```{r}
# 5 categories
model_five <- keras_model_sequential() %>% 
  layer_dense(units = 32, activation = "relu", input_shape = ncol(x_train_five)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = ncol(y_train_five), activation = "softmax")

# 3 categories
model_three <- keras_model_sequential() %>% 
  layer_dense(units = 32, activation = "relu", input_shape = ncol(x_train_three)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = ncol(y_train_three), activation = "softmax")

# 2 categories
model_bi <- keras_model_sequential() %>% 
  layer_dense(units = 32, activation = "relu", input_shape = ncol(x_train_bi)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = ncol(y_train_bi), activation = "sigmoid")
```

Compiling the models.
```{r}
# 5 categories
model_five %>% compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "accuracy")

# 3 categories
model_three %>% compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "accuracy")

# 2 categories
model_bi %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy")
```

## Fitting the models
```{r}
# 5 categories
history_five <- model_five %>% fit(x = x_train_five, 
                               y = y_train_five, 
                               epochs = 10,
                               verbose = 2, 
                               batch_size = 64,
                               validation_split = 0.10, 
                               shuffle = FALSE)

# 3 categories
history_three <- model_three %>% fit(x = x_train_three, 
                               y = y_train_three, 
                               epochs = 10,
                               verbose = 2, 
                               batch_size = 64,
                               validation_split = 0.10, 
                               shuffle = FALSE)

# 2 categories
history_bi <- model_bi %>% fit(x = x_train_bi, 
                               y = y_train_bi, 
                               epochs = 10,
                               verbose = 2, 
                               batch_size = 64,
                               validation_split = 0.10, 
                               shuffle = FALSE)
```


```{r fig.height=4, fig.width=10}
plot_five <- plot(history_five)+
  theme(legend.position = "bottom")+
  ggtitle("model_five")

plot_three <- plot(history_three)+
  theme(legend.position = "bottom")+
  ggtitle("model_three")

plot_bi<- plot(history_bi)+
  theme(legend.position = "bottom")+
  ggtitle("model_bi")

library(ggpubr) #package that helps mixing multiple plots on the same page
plot <- ggarrange(plot_five, plot_three, plot_bi, ncol = 3)

annotate_figure(plot, bottom = text_grob("Loss and accuracy for models", size = 18))
```
## Evaluating on test data

```{r}
# 5 categories
result_five <- model_five %>% evaluate(x_test_five, y_test_five)
result_five <- as.data.frame(result_five)

# 5 categories
result_three <- model_three %>% evaluate(x_test_three, y_test_three)
result_three <- as.data.frame(result_three)

# 5 categories
result_bi <- model_bi %>% evaluate(x_test_bi, y_test_bi)
result_bi <- as.data.frame(result_bi)
```

```{r}
results <- cbind(result_five, result_three, result_bi)
results
```

```{r}
class_pred_five <- model_five %>% predict(x_test_five)
class_pred_three <- model_three %>% predict(x_test_three)
class_pred_bi <- model_bi %>% predict(x_test_bi)
```
Based on the head of the prediction some models seems to predict correctly where as others get the prediction wrong.

To get a more thorough look at the models ability to accurately predict the outcomes the accuracy of the models are calculated.

```{r}
# 5 categories
pred_collected_five <- tibble(
  truth = data_test_five %>% pull(y) %>% as.factor, #Takes the true values from the training data
  predicted_five = model_five %>% predict_classes(x_test_five)) %>%  #predicting y based on the training data
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')

pred_collected_five %<>%
  mutate(.pred = .pred + 1) %>%
  mutate(.pred = ifelse(.pred == 1, "Healthy",
                 ifelse(.pred == 2, "Hoof and limb disorders",
                 ifelse(.pred == 3, "Metabolic disorders",
                 ifelse(.pred == 4, "Reproduction disorders", "Udder disorders"))))) %>%
  mutate(.pred = .pred %>% as.factor())

# 3 categories
pred_collected_three <- tibble(
  truth = data_test_three %>% pull(y) %>% as.factor, #Takes the true values from the training data
  predicted_three = model_three %>% predict_classes(x_test_three)) %>%  #predicting y based on the training data
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')

pred_collected_three %<>%
  mutate(.pred = .pred + 1) %>%
  mutate(.pred = ifelse(.pred == 1, "Healthy",
                 ifelse(.pred == 2, "Other", "Udder disorders"))) %>%
  mutate(.pred = .pred %>% as.factor())

# 2 categories
pred_collected_bi <- tibble(
  truth = data_test_bi %>% pull(y) %>% as.factor, #Takes the true values from the training data
  predicted_bi = model_bi %>% predict_classes(x_test_bi)) %>%  #predicting y based on the training data
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')

pred_collected_bi %<>%
  mutate(.pred = .pred + 1) %>%
  mutate(.pred = ifelse(.pred == 1, "Healthy", "Sick")) %>%
  mutate(.pred = .pred %>% as.factor())
```



Based on the head of the prediction some models seems to predict correctly where as others get the prediction wrong.
```{r}
# 5 categories
pred_five_truth <- pred_collected_five %>%
  count(truth)

pred_five_pred <- pred_collected_five %>%
  count(.pred)

pred_five <- cbind(pred_five_truth, pred_five_pred)
pred_five

# 3 categories
pred_three_truth <- pred_collected_three %>%
  count(truth)

pred_three_pred <- pred_collected_three %>%
  count(.pred)

pred_three <- cbind(pred_three_truth, pred_three_pred)
pred_three

# 2 categories
pred_bi_truth <- pred_collected_bi %>%
  count(truth)

pred_bi_pred <- pred_collected_bi %>%
  count(.pred)

pred_bi <- cbind(pred_bi_truth, pred_bi_pred)
pred_bi
```


```{r fig.width=10}
#5 categories
cm_rf_five <- pred_collected_five %>% conf_mat(truth, .pred)
heatplotfive <- cm_rf_five %>% autoplot(type = "heatmap")

#5 categories
cm_rf_three <- pred_collected_three %>% conf_mat(truth, .pred)
heatplotthree <- cm_rf_three %>% autoplot(type = "heatmap")

#5 categories
cm_rf_bi <- pred_collected_bi %>% conf_mat(truth, .pred)
heatplotbi <- cm_rf_bi %>% autoplot(type = "heatmap")

ggarrange(heatplotbi, heatplotthree)
heatplotfive
```


Since the overall accuracy can be misleading in cases with unbalanced classes the precision for each class in each of the categorizations is calculated.

```{r}
cm5 <- as.matrix(cm_rf_five$table)
cm3 <- as.matrix(cm_rf_three$table)
cm2 <- as.matrix(cm_rf_bi$table)
```


```{r}
rowsums5 = apply(cm5, 1, sum) #Number of predictions per class
colsums5 = apply(cm5, 2, sum) #Number of predictions per class
diag5 = diag(cm5) # Number of correctly classified instances per class

Precision5 = diag5/rowsums5 %>% as.table()
Sensitivity5 = diag5/colsums5 %>% as.table()

cbind(Precision5, Sensitivity5)
```

```{r}
rowsums3 = apply(cm3, 1, sum)
colsums3 = apply(cm3, 2, sum) #Number of predictions per class
diag3 = diag(cm3) 

Precision3 = diag3/rowsums3 %>% as.table()
Sensitivity3 = diag3/colsums3 %>% as.table()

cbind(Precision3, Sensitivity3)
```



```{r}
rowsums2 = apply(cm2, 1, sum)
colsums2 = apply(cm2, 2, sum) #Number of predictions per class
diag2 = diag(cm2)

Precision2 = diag2/rowsums2 %>% as.table()
Sensitivity2 = diag2/colsums2 %>% as.table()

cbind(Precision2, Sensitivity2)
```










































